{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cd ../src"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/disambiguation/entity-linking/disambiguation/src\n"
       ]
      }
     ],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "class Enum(set):\n",
      "    def __getattr__(self, name):\n",
      "        if name in self: return name\n",
      "        raise AttributeError  \n",
      "  \n",
      "SUBTYPES = Enum(\"COMPANY CRIMINAL EDUCATIONAL GEO GOVERNMENT HOSTNAME LOCATION ORGANIZATION PERFORMING PERSON SPORTS\".split())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home = \"/home/disambiguation\"\n",
      "directory = \"data-sets\"\n",
      "target = \"js-1000\"\n",
      "clustering_type = \"k-means\"\n",
      "minibatch = False\n",
      "n_clusters = 12\n",
      "verbose = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import data.collector as cl\n",
      "output_directory = cl.get_output_directory(home, target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Entity(object):\n",
      "  def __init__(self, index, line):\n",
      "    etype, phrase = line.strip().split('\\t')\n",
      "    self.index = index\n",
      "    self.type = etype.strip()\n",
      "    self.phrase = phrase.strip()\n",
      "    \n",
      "  def __str__(self):\n",
      "    return unicode(self).encode('utf-8')\n",
      "  \n",
      "  def __unicode__(self):\n",
      "    return \"[%d] %s (%s)\" % (self.index, self.phrase, self.type)\n",
      "\n",
      "def get_entities(output_directory):\n",
      "    entities = []\n",
      "    index = 0\n",
      "    for line in codecs.open(\"%s/entity\" % output_directory, 'r', 'utf-8'):\n",
      "        entities.append(Entity(index, line))\n",
      "        index += 1\n",
      "    return entities\n",
      "entities = get_entities(output_directory)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "npzfile = np.load(\"%s/data.npz\" % output_directory)\n",
      "data = npzfile['arr_0']\n",
      "indices = npzfile['arr_1']\n",
      "\n",
      "class Info(object):\n",
      "  def __init__(self, index, document_name, document_index, phrase, position, subtype=SUBTYPES.PERSON):\n",
      "    self.index = index\n",
      "    self.document_name = document_name\n",
      "    self.document_index = document_index\n",
      "    self.phrase = phrase\n",
      "    self.position = position\n",
      "    self.subtype = subtype\n",
      "   \n",
      "  def __str__(self):\n",
      "    return unicode(self).encode('utf-8')\n",
      "  \n",
      "  def __unicode__(self):\n",
      "    return \"[%d] %s\\t%s\" % (self.index, self.phrase, self.document_name)\n",
      "\n",
      "def get_info(output_directory):\n",
      "  fp = codecs.open(\"%s/info\" % output_directory, 'r', 'utf-8')\n",
      "  current_doc = None\n",
      "  doc_index = 0\n",
      "  for record in fp:\n",
      "    parts = record.strip().split(\"\\t\")\n",
      "    doc = parts[1]\n",
      "    if current_doc and not current_doc == doc: doc_index += 1\n",
      "    current_doc = doc\n",
      "    yield Info(int(parts[0]), parts[1], doc_index, parts[3], int(parts[4]), parts[2])\n",
      "  fp.close()\n",
      "\n",
      "def filter_info_data(indices):\n",
      "    return [info for info in get_info(output_directory) if info.index in indices]\n",
      "\n",
      "info_data = filter_info_data(indices)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class EntityCountFeature(object):\n",
      "  def __init__(self, entity, count):\n",
      "    self.entity = entity\n",
      "    self.count = count\n",
      "    \n",
      "  def __str__(self):\n",
      "    return unicode(self).encode('utf-8')\n",
      "  \n",
      "  def __unicode__(self):\n",
      "    return \"%s -->> %d\" % (self.entity, int(self.count))\n",
      "    \n",
      "class EntityProximityFeature(object):\n",
      "  def __init__(self, entity, proximity):\n",
      "    self.entity = entity\n",
      "    self.proximity = proximity\n",
      "    \n",
      "  def __str__(self):\n",
      "    return unicode(self).encode('utf-8')\n",
      "  \n",
      "  def __unicode__(self):\n",
      "    return \"%s -->> %.2f\" % (self.entity, self.proximity)\n",
      "    \n",
      "class TopicFeature(object):\n",
      "  def __init__(self, weights):\n",
      "    self.weights = weights\n",
      "    \n",
      "class KeyphraseFeature(object):\n",
      "  def __init__(self, index, phrase):\n",
      "    self.index = index\n",
      "    self.phrase = phrase\n",
      "    \n",
      "class ContextFeature(object):\n",
      "  def __init__(self, index, word):\n",
      "    self.index = index\n",
      "    self.word = word\n",
      "    \n",
      "class Mention(object):\n",
      "\n",
      "  def __init__(self, entities, info, fdata):\n",
      "    #n_entities = len(entities)\n",
      "    self.info = info\n",
      "    self.entity_count_features = []\n",
      "    self.entity_proximity_features = []\n",
      "    for data in fdata:\n",
      "      t, j, value = data.split(\":\")\n",
      "      j = int(j)\n",
      "      value = float(value)\n",
      "      if t == 'p': \n",
      "        self.entity_proximity_features.append(EntityProximityFeature(entities[j], value))\n",
      "      else:\n",
      "        self.entity_count_features.append(EntityCountFeature(entities[j], value))\n",
      "    \n",
      "  def __str__(self):\n",
      "    return unicode(self).encode('utf-8')\n",
      "  \n",
      "  def __unicode__(self):\n",
      "    results = [\"%s\" % feature for feature in self.entity_count_features]\n",
      "    results.extend(\"%s\" % feature for feature in self.entity_proximity_features)\n",
      "    return \"%s:\\n\\t%s\" % (self.info, \"\\n\\t\".join(results))\n",
      "        \n",
      "  def get_phrase(self):\n",
      "        return self.info[index]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_mention_data(indices):\n",
      "    path = \"%s/mention\" % output_directory\n",
      "    mentions = []\n",
      "    position = 0\n",
      "    for line in codecs.open(path, 'r', 'UTF-8'):\n",
      "        parts = line.strip().split('\\t')\n",
      "        index = int(parts[0])\n",
      "        if index in indices:\n",
      "          mentions.append(Mention(entities, info_data[position], parts[1:]))\n",
      "          position += 1  \n",
      "    return mentions\n",
      "mentions = filter_mention_data(indices)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(mentions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "38\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_mention_data(indices, info_data):\n",
      "  path = \"%s/mention\" % output_directory\n",
      "  fp = codecs.open(path, 'r', 'utf-8')\n",
      "  index = 0\n",
      "  mentions = []\n",
      "  for record in fp:\n",
      "    parts = record.strip().split(\"\\t\")\n",
      "    doc = parts[1]\n",
      "    if current_doc and not current_doc == doc: doc_index += 1\n",
      "    current_doc = doc\n",
      "    yield (int(parts[0]), doc_index, parts[3])\n",
      "  fp.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_info = [(index, label) for index, _, label in cl.get_info(output_directory) if index in indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import clustering.domain as cdm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(cdm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "<module 'clustering.domain' from 'clustering/domain.pyc'>"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cmodel = cdm.ClusterModel(n_clusters)\n",
      "cmodel.fit(data)\n",
      "cmodel.dump_cluster_data(data_info, output_directory)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "300/72.0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "4.166666666666667"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "from sklearn import metrics\n",
      "from time import time\n",
      "if minibatch:\n",
      "    km = MiniBatchKMeans(n_clusters=n_clusters, init='k-means++', n_init=1, init_size=1000, batch_size=1000, verbose=verbose)\n",
      "else:\n",
      "    km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1, verbose=verbose)\n",
      "\n",
      "print(\"Clustering %s data with %s\" % (target, km))\n",
      "t0 = time()\n",
      "km.fit(data)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print()\n",
      "\n",
      "#print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "#print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "#print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "#print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
      "#print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels, sample_size=1000))\n",
      "\n",
      "print()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Clustering jr data with KMeans(copy_x=True, init=k-means++, max_iter=100, n_clusters=12, n_init=1,\n",
        "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
        "    verbose=True)\n",
        "Initialization complete\n",
        "Iteration  0, inertia 2407.819\n",
        "Iteration  1, inertia 1652.810\n",
        "Iteration  2, inertia 1597.792"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  3, inertia 1570.819\n",
        "Iteration  4, inertia 1560.376\n",
        "Iteration  5, inertia 1554.515\n",
        "Iteration  6, inertia 1550.383\n",
        "Iteration  7, inertia 1550.072\n",
        "Iteration  8, inertia 1550.011\n",
        "Iteration  9, inertia 1549.988\n",
        "Iteration 10, inertia 1549.980\n",
        "Converged at iteration 10\n",
        "done in 0.087s\n",
        "()\n",
        "()\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(entities)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 65,
       "text": [
        "32536"
       ]
      }
     ],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(data, km.labels_, metric='euclidean'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Silhouette Coefficient: 0.110\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_info = [(index, label) for index, _, label in cl.get_info(output_directory) if index in indices]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "cluster_data = []\n",
      "for index, cluster in enumerate(km.labels_):\n",
      "    cluster_data.append((index, data_info[index][0], data_info[index][1], cluster))\n",
      "    count += 1\n",
      "    #if count == 20: break\n",
      "cluster_data = sorted(cluster_data, key=lambda x: x[3])\n",
      "fp = codecs.open(\"%s/cluster\" % output_directory, 'w', 'utf-8')\n",
      "for index, mindex, label, cluster in cluster_data: fp.write(\"%d\\t%d\\t%s\\t%d\\n\" % (index, mindex, label, cluster))\n",
      "fp.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def summarize_feature_data(cluster, directory, indices, mentions, fp):\n",
      "  summary = {}\n",
      "  subtype_weights = {}\n",
      "  for index in indices:\n",
      "    mention = mentions[index]\n",
      "    phrase = mention.phrase\n",
      "    if not phrase in summary: \n",
      "      summary[phrase] = {}\n",
      "      for subtype in SUBTYPES: summary[phrase][subtype] = {}\n",
      "      subtype_weights[phrase] = {}\n",
      "      for subtype in SUBTYPES: subtype_weights[phrase][subtype] = 0\n",
      "    for featureSet in mention.featureSets:\n",
      "      entity = featureSet.entity\n",
      "      entity_map = summary[phrase][entity.subtype]\n",
      "      if not entity.phrase in entity_map: entity_map[entity.phrase] = 0\n",
      "      entity_map[entity.phrase] += 1\n",
      "      subtype_weights[phrase][entity.subtype] += 1\n",
      "  for phrase in summary.keys():\n",
      "    results = []\n",
      "    for subtype in sorted(SUBTYPES, key=lambda x: -subtype_weights[phrase][x]):\n",
      "      entity_map = summary[phrase][subtype]\n",
      "      keys = sorted(entity_map.keys(), key=lambda x: -entity_map[x])\n",
      "      result = \"; \".join(\"%s:%d\" % (key, entity_map[key]) for key in keys)\n",
      "      results.append(\"%s: %s\" % (subtype, result))\n",
      "    fp.write(\"%d\\t%s\\t%s\\n\" % (cluster, phrase, \"\\t\".join(results)))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dump_feature_data(home, opts, mentions):\n",
      "  print \"dumping feature data\"\n",
      "  data = codecs.open(\"%s/%s/%s\" % (home, opts.directory, \"kmeans-clusters.csv\"), 'r', 'UTF-8')\n",
      "  count = -1\n",
      "  fp = codecs.open(\"%s/%s/%s\" % (home, opts.directory, \"features.csv\"), 'w', 'utf-8')\n",
      "  cluster_map = {}\n",
      "  for line in data: \n",
      "    count += 1\n",
      "    if count == 0: continue\n",
      "    index, doc_id, _, _, cluster = line.strip().split('\\t')\n",
      "    index = int(index)\n",
      "    cluster = int(cluster)\n",
      "    if not cluster in cluster_map: cluster_map[cluster] = set()\n",
      "    cluster_map[cluster].add(index)\n",
      "    mention = mentions[index]\n",
      "    fp.write(\"%d\\t%s\\t%s\\n\" % (cluster, doc_id, mention))\n",
      "  fp.close()\n",
      "  fp = codecs.open(\"%s/%s/cluster-feature-summary.csv\" % (home, opts.directory), 'w', 'utf-8')\n",
      "  for key in cluster_map.keys(): summarize_feature_data(key, opts.directory, cluster_map[key], mentions, fp)\n",
      "  fp.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for n_clusters in range(20,40,1):\n",
      "    km = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1, verbose=False)\n",
      "    km.fit(data)\n",
      "    print(\"%d -->> %0.3f\" % (n_clusters, metrics.silhouette_score(data, km.labels_, metric='euclidean')))  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20 -->> 0.153\n",
        "21 -->> 0.146"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "22 -->> 0.143"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "23 -->> 0.156"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "24 -->> 0.147"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "25 -->> 0.160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "26 -->> 0.157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "27 -->> 0.159"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "28 -->> 0.158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "29 -->> 0.163"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "30 -->> 0.154"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "31 -->> 0.165"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "32 -->> 0.172"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "33 -->> 0.158"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "34 -->> 0.156"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "35 -->> 0.166"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "36 -->> 0.169"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "37 -->> 0.153"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "38 -->> 0.147"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "39 -->> 0.157"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "count = 0\n",
      "for record in cl.get_info(output_directory):\n",
      "    print record\n",
      "    count += 1\n",
      "    if count == 20: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0, 0, u'Chief Justice Roberts')\n",
        "(1, 0, u'Justice Anthony Kennedy')\n",
        "(2, 0, u'Justice Kennedy')\n",
        "(3, 0, u'John Roberts')\n",
        "(4, 0, u'McCutcheon')\n",
        "(5, 0, u'Justice Samuel Alito')\n",
        "(6, 0, u'Buckley')\n",
        "(7, 0, u'Roberts')\n",
        "(8, 0, u'Alito')\n",
        "(9, 0, u'Senate Minority Leader Mitch McConnell')\n",
        "(10, 0, u'Justice Antonin Scalia')\n",
        "(11, 0, u'Clarence Thomas')\n",
        "(12, 0, u'Donald Verrilli')\n",
        "(13, 0, u'Prudence Crowther')\n",
        "(14, 0, u'McCutcheon')\n",
        "(15, 0, u'Barack Obama')\n",
        "(16, 0, u'Justices Antonin Scalia')\n",
        "(17, 0, u'Kagan')\n",
        "(18, 0, u'Joan Biskupic')\n",
        "(19, 0, u'Samuel Alito')\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}